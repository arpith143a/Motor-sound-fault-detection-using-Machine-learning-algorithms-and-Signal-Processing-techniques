{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import random as rn\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "PATH_TRAIN='/home/arpith/Desktop/nectec/sounds/train'\n",
    "LABEL_TO_INDEX_MAP={}\n",
    "\n",
    "###########return classes and assign number to it in training folder which has sound files#############################\n",
    "def init(path):\n",
    "    labels=os.listdir(path) #######list of sub directories in the path\n",
    "    index=0\n",
    "    for label in labels:\n",
    "        LABEL_TO_INDEX_MAP[label]=index   #### initial sub directory name zeroo\n",
    "        index +=1                         #### increments and assigns next number for next sub directory\n",
    "    global NUM_LABELS\n",
    "    NUM_LABELS=len(LABEL_TO_INDEX_MAP)\n",
    "    return LABEL_TO_INDEX_MAP\n",
    "    \n",
    "######################encoding those labels #######################################\n",
    "def one_hot_encoding(label):\n",
    "    encoding=[0]*len(LABEL_TO_INDEX_MAP) ############ assigns zeroes initially for length of LABEL_TO_INDEX_MAP (2 places)\n",
    "    encoding[LABEL_TO_INDEX_MAP[label]]=1  ########### assigns 1 for the presence of the label from LABEL_TO_INDEX_MAP list\n",
    "    return encoding\n",
    "\n",
    "\n",
    "##################### FEATURE EXTRACTION ###################################################################################\n",
    "##################### variance and rms of amplitude(db)############################\n",
    "def get_amp_var_rms(wave_path):\n",
    "    fs_rate, signal = scipy.io.wavfile.read(wave_path)\n",
    "    signal=librosa.amplitude_to_db(signal) ####### changing freq to amplitude in decibels#####\n",
    "    variance=np.var(signal) ######## get variance value of the amplitude values\n",
    "    rms = np.sqrt(np.mean(signal**2))  ########## get rms value of amplitude values\n",
    "    return variance,rms\n",
    "\n",
    "#################### centroid of spectrum formed by stft####################\n",
    "def get_cent(wave_path):\n",
    "    y, sr = librosa.load(wave_path)\n",
    "    S, phase = librosa.magphase(librosa.stft(y))## ############# librosa.stft performs fourier transform on signals\n",
    "    #### librosa.magphase returns magnitude and phase of complex values extracted from librosa.stft\n",
    "    cent=np.mean(librosa.feature.spectral_centroid(y=y, sr=sr, S=None, n_fft=4410, hop_length=2205,\n",
    "                      freq=None).T,axis=0) ### compute spectral centroid of those magnitude values\n",
    "    \n",
    "\n",
    "    rolloff=np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr, S=None, n_fft=4410, hop_length=2205, freq=None, roll_percent=0.85).T)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(y=y,n_fft=4410,S=None,hop_length=2205).T,axis=0)#3\n",
    "    mfccs=np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20,n_fft=4410,hop_length=2205,fmin= 50,n_mels=256).T,axis=0) #1\n",
    "\n",
    "    return mfccs,cent,contrast,rolloff\n",
    "\n",
    "def get_others(wave_path):\n",
    "    y, sr = librosa.load(wave_path)   \n",
    "    stft = np.abs(librosa.stft(y))\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr, S=None, norm=np.inf, n_fft=4410,\n",
    "                hop_length=2205, tuning=None).T,axis=0)#\n",
    "    zero_crossing_rate=np.mean(librosa.feature.zero_crossing_rate(y,frame_length=4410,hop_length=2205).T,axis=0)#5\n",
    "    spectral_flatness=np.mean(librosa.feature.spectral_flatness(y=y,n_fft=4410,S=None,hop_length=2205).T,axis=0)#6\n",
    "\n",
    "    rmse=np.mean(librosa.feature.rmse(y=y,frame_length=4410,S=None,hop_length=2205).T,axis=0)\n",
    "    return chroma,zero_crossing_rate,spectral_flatness,rmse\n",
    "\n",
    "\n",
    "\n",
    "################### extracting mfcc and encode the label #########################\n",
    "# def get_batch(batch_size,path):    \n",
    "path=os.path.join(PATH_TEST,\"*\",'*.wav')  ######## files having path ending with wav\n",
    "waves=gfile.Glob(path) ### collects all the files existing in the folder(including subdirectories having file ended with .wav)\n",
    "\n",
    "\n",
    "#########initialze arrays for each feature###############################\n",
    "X1=[]\n",
    "Y=[]\n",
    "X2=[]\n",
    "X3=[]\n",
    "X4=[]\n",
    "X5=[]\n",
    "X6=[]\n",
    "X7=[]\n",
    "X8=[]\n",
    "X9=[]\n",
    "X10=[]\n",
    "X11=[]\n",
    "Z=[]\n",
    "# mfcc,chroma,melspectrogram and spectral contrast\n",
    "# random.shuffle(waves) #### shuffle the  files( mix of both good and bad files)################\n",
    "\n",
    "##########get features from each wav file##################################\n",
    "for wave_path in waves:\n",
    "    _,label=os.path.split(os.path.dirname(wave_path)) ######splits the subdirectory name from path ##########\n",
    "    #### _= left out path, label=name of sub directory cutted of from path\n",
    "    first,second,third,tenth=(get_others(wave_path))  \n",
    "    fourth,fifth=get_amp_var_rms(wave_path)\n",
    "    sixth,seventh,eight,nineth=get_cent(wave_path)\n",
    "#     sixth=get_cent(wave_path)\n",
    "    X1.append(first)  ### chroma\n",
    "    X2.append(second) ###  zero\n",
    "    X3.append(third)  ### spec flatness\n",
    "    X4.append(fourth) ### amp vars\n",
    "    X5.append(fifth)  ### amp rms\n",
    "    X6.append(sixth)  ### MFCC\n",
    "    X7.append(seventh) ### centroid\n",
    "    X8.append(eight)    ### contrast  \n",
    "    X9.append(nineth)  ### rolloff\n",
    "    X10.append(tenth)  ## rmse\n",
    "   \n",
    "    Y.append(label)\n",
    "\n",
    "########### encode the target values of each wav file##################################\n",
    "def target(arr):\n",
    "    yev=[]\n",
    "    for x in arr:\n",
    "        if x=='good':\n",
    "            yev.append(0)\n",
    "        else:\n",
    "            yev.append(1)\n",
    "    return yev\n",
    "Z=target(Y)\n",
    "\n",
    "######################################## PRE-PROCESSING FEATURES FOR TRAINING ####################################\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "scale=StandardScaler()\n",
    "\n",
    "x1=np.array(X1)\n",
    "x2=np.array(X2)\n",
    "x3=np.array(X3)\n",
    "four=np.array(X4)\n",
    "x4=np.reshape(four, (-1, 1))\n",
    "five=np.array(X5)\n",
    "x5=np.reshape(five, (-1, 1))\n",
    "x6=np.array(X6)\n",
    "\n",
    "x7=np.array(X7)\n",
    "x8=np.array(X8)\n",
    "\n",
    "nine=np.array(X9)\n",
    "x9=np.reshape(nine, (-1, 1))\n",
    "x10=np.array(X10)\n",
    "\n",
    "prenormfeat=np.hstack([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10])\n",
    "\n",
    "normfeat=scale.fit_transform(prenormfeat)\n",
    "minmaxfeat=scaler.fit_transform(prenormfeat)\n",
    "\n",
    "Z=np.array(Z)\n",
    "\n",
    "\n",
    "##################################### normality tests for both min-max and normalizied features#######################\n",
    "from matplotlib import pyplot\n",
    "\n",
    "print(\"histogram data by min-max\")\n",
    "pyplot.hist(minmaxfeat)\n",
    "pyplot.show()\n",
    "print(\"histogram data by standard scaler\")\n",
    "pyplot.hist(normfeat)\n",
    "pyplot.show()\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "print(\"qqplot data by min-max\")\n",
    "qqplot(minmaxfeat, line='s')\n",
    "pyplot.show()\n",
    "\n",
    "print(\"qqplot data by standard scaler\")\n",
    "qqplot(normfeat, line='s')\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "print(\"shapiro data by min-max\")\n",
    "stat, p = shapiro(minmaxfeat)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Sample does not look Gaussian (reject H0)')\n",
    "\n",
    "print(\"shapiro data by standard scaler\")   \n",
    "stat, p = shapiro(normfeat)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Sample does not look Gaussian (reject H0)')\n",
    "    \n",
    "\n",
    "###############################FEATURE SELECTION ##############################################################\n",
    "##################### select best features using information gain ################\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "   \n",
    "selector = SelectKBest(mutual_info_classif, k=46)\n",
    "X_train_clean = selector.fit_transform(normfeat,Z)\n",
    "\n",
    "mask = selector.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "names=['chroma1','chroma2','chroma3','chroma4','chroma5','chroma6','chroma7','chroma8','chroma9','chroma10','chroma11','chroma12',\n",
    "      'zero-cross','spectral-flatness','ampvar','amprms',\n",
    "       'mfcc1','mfcc2','mfcc3','mfcc4','mfcc5','mfcc6','mfcc7','mfcc8','mfcc9','mfcc10','mfcc11','mfcc12','mfcc13','mfcc14','mfcc15',\n",
    "       'mfcc16','mfcc17','mfcc18','mfcc19','mfcc20',\n",
    "      'cent','contrast1','contrast2','contrast3','contrast4','contrast5','contrast6','contrast7','rolloff','rmse']\n",
    "for bool, feature in zip(mask, names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "        \n",
    "#################################applying principal component anaysis#################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "pca = PCA().fit(X_train_clean)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "## finally selected 30 features using principal component analysise, because PCA is also good for selecting features #############\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=30)\n",
    "PCAfeatures=pca.fit_transform(X_train_clean)\n",
    "\n",
    "######################### MODEL TRAINING USING SVM #########################################################################\n",
    "###SVM GridSearchCV##########################\n",
    "from sklearn.svm import SVC\n",
    "tuned_parameters = [{'kernel': ['rbf','linear'], 'gamma': [1e-3, 1e-4, 1e-5 , 1e-6 ,1e-7,1e-8] \n",
    "                     ,'C': [1, 5 , 10 , 100, 1000,10000,100000,1000000]}]\n",
    "# grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=10,\n",
    "                       scoring='accuracy')\n",
    "X=PCAfeatures\n",
    "# X_train_clean\n",
    "clf.fit(X, Z)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
